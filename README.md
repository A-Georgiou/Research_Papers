---

# ğŸ“š Research Papers Collection

Welcome to my research paper repository! Below you'll find an overview of each paper, along with key highlights of their contributions. Each paper focuses on a unique challenge within natural language processing, machine learning, and related fields. Dive in to explore the full potential of each study.

---

### 1. **Evaluating Dependency Parsers**
**Description:**  
This paper provides a comprehensive evaluation of several dependency parsers, examining their performance on different languages and grammatical structures. The aim is to shed light on the strengths and limitations of modern parsers in real-world linguistic contexts.

**Highlights:**
- âš™ï¸ **Comparative Study** of popular dependency parsers, including their accuracy and speed.
- ğŸŒ **Cross-linguistic Analysis** covering a range of typologically diverse languages.
- ğŸ” **Error Analysis** to identify common parsing challenges, such as attachment ambiguities.
- ğŸ“ˆ **Practical Insights** for applying parsers to real-world language processing tasks.

[Link to Paper](./Evaluating%20Dependency%20Parsers.pdf)

---

### 2. **Evidence Reasoning is a Graph Problem**
**Description:**  
This paper redefines evidence reasoning as a graph-theoretic problem, offering a new perspective for improving the precision and explainability of inference processes. The novel approach leverages graph structures to represent relationships between evidence and claims.

**Highlights:**
- ğŸ§  **Graph Representation** of evidence-based reasoning, transforming reasoning steps into vertices and edges.
- ğŸ”„ **Improved Explainability** through graph traversal algorithms that track how evidence supports or contradicts claims.
- ğŸ“Š **Application of Graph Theory** to enhance the accuracy of inference in decision-making systems.
- ğŸ› ï¸ **Framework for Future Research** into graph-based approaches to complex reasoning tasks.

[Link to Paper](./Evidence%20Reasoning%20is%20a%20Graph%20Problem.pdf)

---

### 3. **Explaining and Editing Language Models for Fact-Checking**
**Description:**  
This was my masters thesis, which looked into how we can more efficiently update implicit knowledge stored within pre-trained language models
without doing computationally expensive and highly unstable fine-tuning through the use of a hyper-network. The research focused on the task of fact-checking, improving model performance at implicit claim verification, as well as producing
new techniques to generate explanations.
It offers a toolkit for understanding model decisions and provides methods to correct outputs when they deviate from factual information.


**Highlights:**
- ğŸ› ï¸ **Toolset for Explaining Decisions** made by large language models in fact-checking scenarios.
- ğŸ“ **Model Editing Techniques** that allow fine-tuning or correcting predictions in real-time without extensive retraining.
- ğŸš¨ **Addressing Misinformation** by enabling fact-checkers to directly intervene in model outputs.
- ğŸ“š **Case Studies** demonstrating the effectiveness of these methods in reducing false information propagation.

[Link to Paper](./Explaining%20and%20Editing%20Language%20Models%20for%20Fact-Checking.pdf)

---

### 4. **How Important is Contextual Understanding in Spam Filtering**
**Description:**  
This study explores the significance of contextual understanding in detecting spam, revealing that context-aware models outperform traditional keyword-based techniques in distinguishing legitimate messages from spam.

**Highlights:**
- ğŸ’¡ **Contextual Models** outperform simple keyword-based filtering approaches.
- ğŸ§ª **Experimental Comparison** of deep learning models that leverage context versus traditional methods.
- ğŸ“‰ **Reduction in False Positives** by using models that understand the broader message context, not just individual words.
- ğŸ“Š **Data Analysis** on real-world spam datasets, including insights on phishing detection.

[Link to Paper](./How%20Important%20is%20Contextual%20Understanding%20in%20Spam%20Filtering.pdf)

---

### 5. **WNUT17 Shared Task: Analysis of Language Models on The Task of Named Entity Recognition**
**Description:**  
This paper analyzes the performance of various language models on the WNUT17 Shared Task for Named Entity Recognition (NER), particularly focusing on noisy text environments, such as social media posts and informal communication.

**Highlights:**
- ğŸ¤– **Detailed Model Comparison** on handling noisy text and unseen entities in NER tasks.
- ğŸŒ **Robustness in Noisy Environments** by testing models on social media data with spelling errors, slang, and informal expressions.
- ğŸ§  **Insights into Generalization** capabilities of language models when encountering unseen or rare entities.
- ğŸ“Š **Evaluation Metrics** that measure model effectiveness beyond standard benchmarks.

[Link to Paper](./WNUT17%20Shared%20Task%20-%20Analysis%20of%20Language%20Models%20on%20The%20Task%20of%20Named%20Entity%20Recognition.pdf)

---

### ğŸ“œ **Repository Overview**
This collection represents a broad spectrum of research contributions in fields such as natural language processing, machine learning, and information retrieval. Each paper highlights innovative approaches and offers tools and insights for researchers and practitioners alike.

Feel free to explore the papers and reach out for discussions or collaborations!

---
